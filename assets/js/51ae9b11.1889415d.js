"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[812],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>c});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=i.createContext({}),u=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return i.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=u(n),c=a,h=m["".concat(s,".").concat(c)]||m[c]||d[c]||r;return n?i.createElement(h,o(o({ref:t},p),{},{components:n})):i.createElement(h,o({ref:t},p))}));function c(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var u=2;u<r;u++)o[u]=n[u];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5575:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var i=n(7462),a=(n(7294),n(3905));const r={title:"Cluster GPU Configuration"},o=void 0,l={unversionedId:"setup/gpu",id:"setup/gpu",title:"Cluster GPU Configuration",description:"Automatic GPU Operator Configuration",source:"@site/docs/setup/gpu.md",sourceDirName:"setup",slug:"/setup/gpu",permalink:"/setup/gpu",draft:!1,editUrl:"https://github.com/rancher/opni-docs/edit/main/docs/setup/gpu.md",tags:[],version:"current",frontMatter:{title:"Cluster GPU Configuration"}},s={},u=[{value:"Automatic GPU Operator Configuration",id:"automatic-gpu-operator-configuration",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"GPU Node Hardware Requirements:",id:"gpu-node-hardware-requirements",level:3},{value:"Kubernetes Cluster Requirements:",id:"kubernetes-cluster-requirements",level:3},{value:"Install Custom Resources",id:"install-custom-resources",level:2},{value:"GPUPolicyAdapter",id:"gpupolicyadapter",level:3},{value:"Provider-specific Notes and Troubleshooting",id:"provider-specific-notes-and-troubleshooting",level:2},{value:"K3S",id:"k3s",level:3},{value:"RKE",id:"rke",level:3},{value:"VGPU",id:"vgpu",level:2},{value:"Other Notes",id:"other-notes",level:2}],p={toc:u};function d(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,i.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"automatic-gpu-operator-configuration"},"Automatic GPU Operator Configuration"),(0,a.kt)("p",null,"Opni can utilize GPU acceleration to enable log anomaly detection for workload\nand application logs. If you would like Opni to learn from your own workloads,\nfollow the instructions below to configure your cluster. If GPU acceleration\nis not enabled, Opni can still analyze your Control Plane logs using our\npretrained models."),(0,a.kt)("p",null,"Opni bundles its own modified version of the Nvidia GPU Operator which is the\nrecommended way to enable GPU acceleration. It may be possible to install the\nupstream GPU Operator via helm chart, but this is not recommended, as it will\nlikely not work with Opni. Follow the instructions below to enable GPU\nacceleration using Opni's built-in GPU Operator."),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,a.kt)("h3",{id:"gpu-node-hardware-requirements"},"GPU Node Hardware Requirements:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"1x NVIDIA GPU",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"If not using VGPU, any Quadro or Tesla GPU will work. "),(0,a.kt)("li",{parentName:"ul"},"If using VGPU, one of the following GPUs is required:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Tesla M6, M10, M60"),(0,a.kt)("li",{parentName:"ul"},"Tesla P4, P6, P40, P100"),(0,a.kt)("li",{parentName:"ul"},"Tesla V100"),(0,a.kt)("li",{parentName:"ul"},"Quadro RTX 6000/8000"),(0,a.kt)("li",{parentName:"ul"},"Tesla T4"),(0,a.kt)("li",{parentName:"ul"},"NVIDIA A10, A16, A30, A40, A100, RTX A5000/A6000"))),(0,a.kt)("li",{parentName:"ul"},"MIG is not supported when using automatic GPU Operator configuration at this time."))),(0,a.kt)("li",{parentName:"ul"},"Ubuntu 20.04. No other host OS is supported at this time."),(0,a.kt)("li",{parentName:"ul"},"The GPU node should not have an Nvidia driver or container runtime installed.\nThe GPU Operator handles installing all the necessary drivers automatically.")),(0,a.kt)("h3",{id:"kubernetes-cluster-requirements"},"Kubernetes Cluster Requirements:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Vanilla upstream Kubernetes, or one of the following distributions:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"RKE"),(0,a.kt)("li",{parentName:"ul"},"K3S >= v1.22.2+k3s1"),(0,a.kt)("li",{parentName:"ul"},"RKE2"),(0,a.kt)("li",{parentName:"ul"},"Amazon EKS"),(0,a.kt)("li",{parentName:"ul"},"Google GKE"),(0,a.kt)("li",{parentName:"ul"},"Azure AKS"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Opni Operator installed")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Cert-Manager installed ")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"NFD installed (see deploy/examples/nfd_aio.yaml in the Opni repo)"))),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"install-custom-resources"},"Install Custom Resources"),(0,a.kt)("h3",{id:"gpupolicyadapter"},"GPUPolicyAdapter"),(0,a.kt)("p",null,"Creating a GPUPolicyAdapter resource will trigger the GPU Operator to start\nconfiguring the cluster. "),(0,a.kt)("p",null,"If not using VGPU, the GPUPolicyAdapter resource does not need any customization,\nand can be created with an empty spec, as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: opni.io/v1beta1\nkind: GpuPolicyAdapter\nmetadata:\n  name: gpu\nspec: {}\n")),(0,a.kt)("p",null,"(note that the empty brackets after ",(0,a.kt)("inlineCode",{parentName:"p"},"spec:")," are important)"),(0,a.kt)("p",null,"If using VGPU, please follow the official documentation to build the VGPU driver\nimage, and create a GPUPolicyAdapter resource with the following spec, filling\nin the ",(0,a.kt)("inlineCode",{parentName:"p"},"driver")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"licenseConfigMap")," fields:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: opni.io/v1beta1\nkind: GpuPolicyAdapter\nmetadata:\n  name: vgpu\nspec:\n  images:\n    driver: # Image name for the VGPU driver\n  vgpu:\n    licenseConfigMap: # ConfigMap containing gridd.conf and the client config token\n    licenseServerKind: nls \n")),(0,a.kt)("p",null,"Once the GPUPolicyAdapter resource is created, the GPU Operator will begin\nconfiguring the cluster. This takes a few minutes, and the container engine\nwill be restarted during this process. Once all the pods in the ",(0,a.kt)("inlineCode",{parentName:"p"},"gpu-operator-resources"),"\nnamespace complete, GPUs will be available for use and the Opni GPU Controller\npod should start."),(0,a.kt)("h2",{id:"provider-specific-notes-and-troubleshooting"},"Provider-specific Notes and Troubleshooting"),(0,a.kt)("h3",{id:"k3s"},"K3S"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"If you are using K3S, you must use v1.22.2+k3s1 or later. This version\nintroduces automatic detection of Nvidia container runtimes on nodes, and\na large number of bugfixes related to pod lifecycle were added in\nupstream Kubernetes for this release. If you are using an earlier version,\nyou may experience issues where pods become stuck in the 'Terminating'\nstate. If this happens, force-deleting the stuck pods should help resolve\nthe issue.")),(0,a.kt)("h3",{id:"rke"},"RKE"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"RKE uses Docker as its container engine. If you are using RKE, you should\nbe aware of how the Nvidia runtime interacts with Docker. When the GPU\noperator is installed for the first time, it will create a ",(0,a.kt)("inlineCode",{parentName:"p"},"RuntimeClass"),"\nobject which allows Kubernetes to use the ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia")," runtime, which is\nconfigured in the docker engine itself. The RuntimeClass will look similar\nto this when using RKE:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: node.k8s.io/v1alpha1\nkind: RuntimeClass\nmetadata:\n  name: nvidia\nhandler: docker\n")),(0,a.kt)("p",{parentName:"li"},"The RuntimeClass ",(0,a.kt)("inlineCode",{parentName:"p"},"name")," is used in a pod spec to identify the runtime to use.\nIt is specified using the ",(0,a.kt)("inlineCode",{parentName:"p"},"runtimeClassName")," field:"),(0,a.kt)("pre",{parentName:"li"},(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nspec:\n  # ...\n  runtimeClassName: nvidia\n  # ...\n")),(0,a.kt)("p",{parentName:"li"},"When using containerd for example, the ",(0,a.kt)("inlineCode",{parentName:"p"},"handler")," field in the ",(0,a.kt)("inlineCode",{parentName:"p"},"RuntimeClass"),"\nis used to select a specific container runtime by name. However, dockershim\ndoes not support selecting custom container runtimes, meaning the nvidia\ncontainer runtime ",(0,a.kt)("em",{parentName:"p"},"must")," be set as the default. The GPU Operator will still\nutilize a ",(0,a.kt)("inlineCode",{parentName:"p"},"RuntimeClass"),", but its ",(0,a.kt)("inlineCode",{parentName:"p"},"handler")," must be set to ",(0,a.kt)("inlineCode",{parentName:"p"},"docker")," to\nwork. Otherwise, you may see an error like this:"),(0,a.kt)("blockquote",{parentName:"li"},(0,a.kt)("p",{parentName:"blockquote"},'Failed to create pod sandbox: rpc error: code = Unknown desc = RuntimeHandler "nvidia" not supported')),(0,a.kt)("p",{parentName:"li"},"If you encounter this error, check the ",(0,a.kt)("inlineCode",{parentName:"p"},"handler")," field in the ",(0,a.kt)("inlineCode",{parentName:"p"},"RuntimeClass"),"\nand ensure it is set to ",(0,a.kt)("inlineCode",{parentName:"p"},"docker"),". "))),(0,a.kt)("h2",{id:"vgpu"},"VGPU"),(0,a.kt)("p",null,"If you are using VGPUs, you should be aware of the following:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Nvidia gridd ",(0,a.kt)("em",{parentName:"p"},"does not")," run on the virtualization host. Instead, it runs in\nguest VMs - when using the GPU Operator, gridd runs inside the nvidia driver\ndaemonset pod.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The VGPU driver must match the driver on the host. When downloading the\ndriver package from the Nvidia License Portal, you will be presented with\ntwo driver runfiles, for example:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"NVIDIA-Linux-x86_64-470.63-vgpu-kvm.run")," - Installed on the ",(0,a.kt)("strong",{parentName:"li"},"host")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"NVIDIA-Linux-x86_64-470.63.01-grid.run")," - Installed on the ",(0,a.kt)("strong",{parentName:"li"},"guest")," (automatic with GPU operator)")),(0,a.kt)("p",{parentName:"li"},"Ensure the correct drivers are used for the host and guest. ")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"CUDA drivers do not need to be installed on the virtualization host (and are not\nincluded by default with the VGPU driver)")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"VGPU mediated device UUIDs are not persistent across reboots. This can cause\nlibvirt to fail after a reboot if you are running a VM with VGPUs. To fix this:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"List instances on the host with ",(0,a.kt)("inlineCode",{parentName:"p"},"virsh list --all")," (the instance will be shut off)")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Find the instance that has the VGPU. If you don't know which one it is,\nuse the ",(0,a.kt)("inlineCode",{parentName:"p"},"virsh edit")," command to view the XML configuration of each instance\nand search for ",(0,a.kt)("inlineCode",{parentName:"p"},"mdev"),".")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Undefine the instance using ",(0,a.kt)("inlineCode",{parentName:"p"},"virsh undefine")," and restart libvirtd.")))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"You might not be able to determine from the host whether a guest VGPU\nis licensed. To check license status, you should open a shell to a pod\nrunning with the nvidia container runtime and use ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia-smi")," from there.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"On the virtualization host, you may notice that your physical GPU is bound to\nthe ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia")," driver, even though the ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia_vgpu_vfio")," driver is available.\nThe official docs are ambiguous on this, but your GPU should be bound to\nthe ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia")," driver. The guest VGPU will also be bound to the ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia")," driver\nonce the guest drivers are installed."))),(0,a.kt)("h2",{id:"other-notes"},"Other Notes"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia.com/gpu")," resource request behaves counterintuitively. It serves as\na node scheduling hint for pods, and to keep track of which/how many pods are\nusing GPUs. ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia.com/gpu")," a first-class node resource just like cpu and\nmemory. If a pod makes a request for one, the node it is scheduled on must\nhave that resource available. The existence of this resource request does\nnot equate to a pod taking exclusive ownership of the device. In fact, a pod\ncan use a GPU without requesting an ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia.com/gpu")," resource at all, as long\nas it has the correct environment variables and ",(0,a.kt)("inlineCode",{parentName:"p"},"runtimeClassName")," set.\nThis is how the GPU Operator pods are configured - they do not request GPU\nresources by design, rather they are scheduled onto GPU nodes by other means\nand set up to use the nvidia container runtime. ")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Regardless of ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia.com/gpu")," resource requests or container runtime, GPUs\nwill ",(0,a.kt)("em",{parentName:"p"},"only")," be available if the ",(0,a.kt)("inlineCode",{parentName:"p"},"NVIDIA_VISIBLE_DEVICES")," and\n",(0,a.kt)("inlineCode",{parentName:"p"},"NVIDIA_DRIVER_CAPABILITIES")," environment variables are set correctly. They\nshould be set as follows:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"NVIDIA_VISIBLE_DEVICES=all"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"NVIDIA_DRIVER_CAPABILITIES=compute,utility")),(0,a.kt)("p",{parentName:"li"},"They can be set either in the pod spec, or in the docker image itself\nby using ",(0,a.kt)("inlineCode",{parentName:"p"},"ENV")," in the Dockerfile.")))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"If you are not using the GPU operator, but you are using the nvidia device\nplugin daemonset by itself, be aware that you must patch the daemonset to\ninclude ",(0,a.kt)("inlineCode",{parentName:"p"},"runtimeClassName: nvidia")," in its pod template. The GPU operator will\ndo this automatically, but the daemonset does not have it set by default.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The GPU Operator installs the drivers in such a way that they are not\ndirectly visible or usable by the host. As such, tools like ",(0,a.kt)("inlineCode",{parentName:"p"},"nvidia-smi")," will\nnot be available, so you will need to exec into a pod running with the nvidia\ncontainer runtime to interact with a GPU.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"A common way to pass a GPU through to a virtual machine is by using the\nvfio-pci driver. There are several tutorials online that explain how to set\nup vfio-pci. It is important to note that some Nvidia GPUs expose additional\nPCI devices (such as an audio or usb controller) in the same IOMMU group\nas the GPU device itself. When binding the GPU device to the vfio-pci driver,\nensure that any additional devices belonging to the GPU are bound as well."))))}d.isMDXComponent=!0}}]);